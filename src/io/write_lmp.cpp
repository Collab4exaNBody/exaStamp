#include <exanb/core/basic_types_yaml.h>
#include <exanb/core/basic_types.h>
#include <exanb/core/operator.h>
#include <exanb/core/operator_slot.h>
#include <exanb/core/operator_factory.h>
#include <exanb/core/parallel_grid_algorithm.h>
#include <exanb/core/make_grid_variant_operator.h>
#include <exanb/core/grid.h>
#include <exanb/core/basic_types_stream.h>
#include <exanb/core/domain.h>
#include <exanb/core/string_utils.h>
#include <exaStamp/particle_species/particle_specie.h>
#include <exanb/fields.h>


#include <mpi.h>
#include <string>
#include <cstring>
#include <iomanip>

namespace exaStamp
{
  
  using namespace exanb;
  
  template< class GridT
/*           ,class = AssertGridHasFields< GridT , field::_rx0, field::_ry0, field::_rz0 > */
           >
  class WriteLMP : public OperatorNode
  {
    //field type
    using has_type_field_t = typename GridT::CellParticles::template HasField < field::_type > ;
    static constexpr bool has_type_field = has_type_field_t::value;

    //field id
    using has_id_field_t = typename GridT::CellParticles::template HasField < field::_id > ;
    static constexpr bool has_id_field = has_id_field_t::value;
        
    ADD_SLOT( MPI_Comm           , mpi                 , INPUT , REQUIRED );
    ADD_SLOT( GridT              , grid                , INPUT , REQUIRED );
    ADD_SLOT( Domain             , domain              , INPUT , REQUIRED );
    ADD_SLOT( std::string        , filename            , INPUT , std::string("output.lmp") , DocString{"File name"} );
    ADD_SLOT( long               , timestep            , INPUT , REQUIRED );
    ADD_SLOT( ParticleSpecies    , species             , INPUT , REQUIRED );

  public:
  
    inline void execute () override final
    {
      static const char* data_line_format = "%12llu %5u  % .16e % .16e % .16e ";
      static const std::string data_line_ref = format_string(data_line_format, 1 , 1 , -1.0e103 , -1.0e103 , -1.0e103 );
      static const size_t data_line_size = data_line_ref.length();

      static const char* vel_line_format = "%12llu % .16e % .16e % .16e ";
      static const std::string vel_line_ref = format_string(vel_line_format, 1 , -1.0e103 , -1.0e103 , -1.0e103 );
	    static const size_t vel_line_size = vel_line_ref.length();

      size_t n_cells = grid->number_of_cells();
      auto cells = grid->cells();
      
      int rank=0, np=1;
      MPI_Comm_rank(*mpi, &rank);
      MPI_Comm_size(*mpi, &np);

      // structure for file opening/writing in mpi
      MPI_File mpiFile;
      MPI_Status status;

      // all the processors open the .xyz file
      MPI_File_open(MPI_COMM_WORLD, filename->c_str(), MPI_MODE_CREATE | MPI_MODE_WRONLY, MPI_INFO_NULL, &mpiFile);

      // count total number of particles among all processors
      unsigned long long local_nb_particles = 0;
      for(size_t c=0; c<n_cells;++c)
      {
        if( !grid->is_ghost_cell(c) ) { local_nb_particles += cells[c].size(); }
      }
      unsigned long long total_nb_particles = 0;
      MPI_Allreduce(&local_nb_particles,&total_nb_particles,1,MPI_UNSIGNED_LONG_LONG,MPI_SUM,*mpi);

      unsigned long long global_atom_start = 0;
      MPI_Scan( &local_nb_particles , &global_atom_start , 1 , MPI_UNSIGNED_LONG_LONG,MPI_SUM,*mpi);
      global_atom_start -= local_nb_particles;

/*
# Comment
         250  atoms
           1  atom types

      0.000000000000      15.900000000000  xlo xhi
      0.000000000000      15.900000000000  ylo yhi
      0.000000000000      15.900000000000  zlo zhi

Masses

            1   95.96000000             # Mo

Atoms # atomic

         1    1        0.020231084146      -0.097390034307      -0.066070146532
         2    1        1.651889840283       1.626009438192       1.629095312840
         3    1        3.131735903643       0.065105353084       0.068857916525
         4    1        4.734320986725       1.677784955352       1.521746136731
         5    1        6.341162438047       0.015294909821       0.009493644022
         6    1        7.973692780982       1.653465446155       1.677555723059
         7    1        9.446434219017      -0.039055799623       0.060633942544
         8    1       11.144519824297       1.577595207823       1.523513553354
...

Velocities

  1 vx vy vz
  ...
  ...                
  N vx vy vz               

*/

      auto [ xlo , ylo , zlo ] = domain->xform() * domain->origin();
      auto [ xhi , yhi , zhi ] = domain->xform() * domain->extent();

      std::string domain_header = format_string("# Generated by exaStamp V3\n%12llu  atoms\n%12llu  atom types\n\n%.12f\t%.12f  xlo xhi\n%.12f\t%.12f  ylo yhi\n%.12f\t%.12f  zlo zhi\n\n",
                     static_cast<unsigned long long>(total_nb_particles), static_cast<unsigned long long>(species->size()),xlo,xhi,ylo,yhi,zlo,zhi);

      std::string species_header = "Masses\n";
      int sp_count = 0;
      for(const auto& s : *species) { species_header += format_string("%12llu %.12f # %s\n",++sp_count,s.m_mass,s.name()); }
      species_header += "\n\nAtoms # atomic\n\n";
      
      auto header = domain_header + species_header;

      unsigned long long offset_header = header.length();

      // only processor 0 writes the header
      if (rank==0)
      {
        ldbg << "LMP: write header"<<std::endl;
      	MPI_File_write_at(mpiFile, 0 , header.data() , header.length() , MPI_CHAR , &status);
      }
      
	    // Write atom positions
      ldbg << "LMP: write positions @"<<offset_header<< std::endl;
	    unsigned long long atom_count = 0;
	    for(size_t c=0; c<n_cells;++c)
      {
        if( !grid->is_ghost_cell(c) )
        {
	        const auto * __restrict__ rx = cells[c][field::rx];
	        const auto * __restrict__ ry = cells[c][field::ry];
	        const auto * __restrict__ rz = cells[c][field::rz];
	        const uint8_t* __restrict__ types = cells[c].field_pointer_or_null(field::type);
	        
		      np = cells[c].size();
		      for(int p=0;p<np;++p)
	        {
	          Vec3d r = domain->xform() * Vec3d{rx[p],ry[p],rz[p]};
	          //const char* type_name = "XX";
	          int atom_type = 0;
	          if( types != nullptr ) atom_type = types[p];
			        
	          auto data_line = format_string(data_line_format, global_atom_start+atom_count+1 , atom_type+1 , r.x,r.y,r.z );
	          assert( data_line.length() <= data_line_size );
	          assert( !data_line.empty() );
	          assert( data_line.back() == ' ' );
	          data_line.resize(data_line_size,' ');
	          data_line.back() = '\n';
	          assert( data_line.length() == data_line_size );
	      
	          unsigned long long offset = offset_header + ( global_atom_start + atom_count ) * data_line_size;
	          MPI_File_write_at( mpiFile, offset, data_line.data(), data_line.length() , MPI_CHAR , &status );
	          
	          ++ atom_count;
	        }
	      }
      }
      
      const std::string velocity_header = "\nVelocities\n\n";
      const size_t offset_velocities_header = offset_header + total_nb_particles * data_line_size;
      ldbg << "LMP: write velocities header @"<<offset_velocities_header<< std::endl;
      MPI_File_write_at( mpiFile, offset_velocities_header, velocity_header.data(), velocity_header.length() , MPI_CHAR , &status );
      const size_t offset_velocities = offset_velocities_header + velocity_header.length();
      
	    // Write atom velocities
      ldbg << "LMP: write velocities @"<<offset_velocities<< std::endl;
	    atom_count = 0;
	    for(size_t c=0; c<n_cells;++c)
      {
        if( !grid->is_ghost_cell(c) )
        {
	        const auto * __restrict__ vx = cells[c][field::vx];
	        const auto * __restrict__ vy = cells[c][field::vy];
	        const auto * __restrict__ vz = cells[c][field::vz];
	        
		      np = cells[c].size();
		      for(int p=0;p<np;++p)
	        {
	          auto data_line = format_string(vel_line_format, global_atom_start+atom_count+1 , vx[p] , vy[p] , vz[p] );
	          assert( data_line.length() <= vel_line_size );
	          assert( !data_line.empty() );
	          assert( data_line.back() == ' ' );
	          data_line.resize(vel_line_size,' ');
	          data_line.back() = '\n';
	          assert( data_line.length() == vel_line_size );
	      
	          unsigned long long offset = offset_velocities + ( global_atom_start + atom_count ) * vel_line_size;
	          MPI_File_write_at( mpiFile, offset, data_line.data(), data_line.length() , MPI_CHAR , &status );
	          
	          ++ atom_count;
	        }
	      }
      }

      MPI_File_close(&mpiFile);
    }
    
};

  template<class GridT> using WriteLMPTmpl = WriteLMP<GridT>;
  
  // === register factories ===  
  CONSTRUCTOR_FUNCTION
  {
    OperatorNodeFactory::instance()->register_factory( "write_lmp", make_grid_variant_operator< WriteLMPTmpl > );
  }

}
